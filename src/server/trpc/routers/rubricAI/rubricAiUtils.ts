import type { AiTool } from "../../../../utils/aiUtils/createAiTool";
import { executeToolCall } from "../../../../utils/aiUtils/executeToolCall";
import { z } from "zod";
import {
  getMetadataSubmissionDirectory,
  sanitizeName,
} from "../canvas/canvasStorageUtils";
import fs from "fs";
import path from "path";
import {
  type AnalysisResult,
  AnalysisResultSchema,
  type FullEvaluation,
  FullEvaluationSchema,
  type ConversationMessage,
  evidenceSchemaPrompt,
} from "./rubricAiReportModels";
import OpenAI from "openai";
import { getAiCompletion } from "../../../../utils/aiUtils/getAiCompletion";
import { aiModel } from "../../../../utils/aiUtils/getOpenaiClient";
import { parseSchema } from "../parseSchema";

// Helper functions to convert between domain model and OpenAI types
export function toOpenAIMessage(
  message: ConversationMessage
): OpenAI.Chat.ChatCompletionMessageParam {
  const baseMessage: Record<string, unknown> = {
    role: message.role,
  };

  if (message.content) {
    // Handle both string content and structured content
    if (typeof message.content === "string") {
      baseMessage.content = message.content;
    } else {
      // Handle array content (for images and mixed content)
      baseMessage.content = message.content.map((item) => {
        if (item.type === "text") {
          return {
            type: "text" as const,
            text: item.text || "",
          };
        } else if (item.type === "image_url") {
          // Convert base64 data to data URL for OpenAI
          const mediaType = item.image_url?.mediaType || "image/png";
          const dataUrl = `data:${mediaType};base64,${
            item.image_url?.base64 || ""
          }`;
          return {
            type: "image_url" as const,
            image_url: {
              url: dataUrl,
            },
          };
        }
        return item;
      });
    }
  }

  if (message.tool_calls && message.tool_calls.length > 0) {
    baseMessage.tool_calls = message.tool_calls.map((tc) => ({
      id: tc.id || "",
      type: "function" as const,
      function: {
        name: tc.function?.name || "",
        arguments: tc.function?.arguments || "",
      },
    }));
  }

  if (message.tool_call_id) {
    baseMessage.tool_call_id = message.tool_call_id;
  }

  return baseMessage as unknown as OpenAI.Chat.ChatCompletionMessageParam;
}

// Function to parse a resultSchema object from a ConversationMessage
export function parseResultFromMessage<T>(
  message: ConversationMessage,
  resultSchema: z.ZodType<T>
): T {
  // Parse the final result
  if (!message.content) {
    console.log(message);
    throw new Error("No content in final response from AI service");
  }

  // Convert content to string if it's an array (shouldn't happen for structured responses, but safety check)
  const contentStr =
    typeof message.content === "string"
      ? message.content
      : JSON.stringify(message.content);

  const parsedResult = JSON.parse(contentStr);
  console.log("ðŸ“‹ Parsed AI response:", JSON.stringify(parsedResult, null, 2));

  // Validate the result against the schema
  try {
    const result = parseSchema(
      resultSchema,
      parsedResult,
      "AI response validation"
    );

    return result;
  } catch (error) {
    console.error(
      "Failed to parse AI response:",
      JSON.stringify(message, null, 2)
    );
    throw error;
  }
}

export function fromOpenAIMessage(openaiMessage: unknown): ConversationMessage {
  const msg = openaiMessage as Record<string, unknown>;

  // Handle content conversion from OpenAI format to domain model
  let content: ConversationMessage["content"];
  if (typeof msg.content === "string") {
    content = msg.content;
  } else if (Array.isArray(msg.content)) {
    // Convert OpenAI structured content to our domain model
    content = msg.content.map((item: Record<string, unknown>) => {
      if (item.type === "text") {
        return {
          type: "text" as const,
          text: item.text as string | undefined,
        };
      } else if (item.type === "image_url") {
        // Extract base64 data from data URL if present
        const imageUrl = item.image_url as { url?: string } | undefined;
        const url = imageUrl?.url || "";

        // Parse data URL to extract base64 and media type
        const dataUrlMatch = url.match(/^data:([^;]+);base64,(.+)$/);
        if (dataUrlMatch) {
          return {
            type: "image_url" as const,
            image_url: {
              base64: dataUrlMatch[2],
              mediaType: dataUrlMatch[1],
            },
          };
        } else {
          // Fallback for non-data URLs (shouldn't happen in our use case)
          return {
            type: "image_url" as const,
            image_url: {
              base64: "",
              mediaType: "image/png",
            },
          };
        }
      }
      return {
        type: item.type as "text" | "image_url",
        text: item.text as string | undefined,
        image_url: undefined,
      };
    });
  } else {
    content = undefined;
  }

  return {
    role: msg.role as ConversationMessage["role"],
    content,
    tool_calls: (msg.tool_calls as unknown[])?.map((tc: unknown) => {
      const toolCall = tc as Record<string, unknown>;
      return {
        id: toolCall.id as string,
        type: toolCall.type as string,
        function: toolCall.function as { name: string; arguments?: string },
      };
    }),
    tool_call_id: msg.tool_call_id as string,
  };
}

export async function* getRubricAnalysisConversation({
  startingMessages,
  tools,
  resultSchema,
}: {
  startingMessages: ConversationMessage[];
  tools: AiTool[];
  resultSchema: z.ZodTypeAny;
}): AsyncGenerator<ConversationMessage> {
  const conversationMessages: ConversationMessage[] = [...startingMessages];
  const maxRounds = 10; // Prevent infinite loops
  let round = 0;

  while (round < maxRounds) {
    round++;

    // Use the reusable completion function
    const assistantMessage = await getAiCompletion({
      messages: conversationMessages,
      tools,
      responseFormat: resultSchema,
      temperature: 0.1,
    });

    conversationMessages.push(assistantMessage);
    yield assistantMessage;

    if (assistantMessage.tool_calls && assistantMessage.tool_calls.length > 0) {
      // Execute tool calls and add results
      const toolMessages = await Promise.all(
        assistantMessage.tool_calls
          .filter((tc) => tc.id && tc.function?.name) // Filter out incomplete tool calls
          .map((toolCall) => {
            // Use the tool call directly with the generic domain model
            const genericToolCall = {
              id: toolCall.id!,
              type: toolCall.type || "function",
              function: {
                name: toolCall.function!.name,
                arguments: toolCall.function!.arguments || "",
              },
            };
            return executeToolCall(genericToolCall, tools);
          })
      );

      // Add tool messages directly to conversation and yield them
      for (const msg of toolMessages) {
        const toolMessage = {
          role: msg.role,
          content: msg.content,
          tool_call_id: msg.tool_call_id,
        };
        conversationMessages.push(toolMessage);
        yield toolMessage;
      }
    } else {
      // No tool calls means we're done with this exploration round

      return;
    }

    const continueMessage = {
      role: "user" as const,
      content: `Continue your analysis if you need more information, or provide your final structured JSON analysis when you have gathered enough evidence.`,
    };
    conversationMessages.push(continueMessage);
    yield continueMessage;
  }

  // Add final prompt for structured output
  const finalPromptMessage = {
    role: "user" as const,
    content: `Please provide your final analysis of how well this submission meets the rubric criterion. Base your assessment on the files you've examined and provide specific evidence to support your evaluation.`,
  };
  conversationMessages.push(finalPromptMessage);
  yield finalPromptMessage;

  console.log(
    "About to call OpenAI with zodResponseFormat for structured output"
  );

  // Use the reusable completion function with structured output
  const finalMessage = await getAiCompletion({
    messages: conversationMessages,
    responseFormat: resultSchema,
    temperature: 0.1,
  });

  // Add the final response to conversation and yield it
  conversationMessages.push(finalMessage);
  yield finalMessage;

  // Parse the final result using the extracted function
  // const result = parseResultFromMessage(finalMessage, resultSchema);

  // Return both conversation and result
  // return {
  //   conversation: conversationMessages,
  //   result,
  // };
}

// Generator utility function that yields messages and returns analysis
export async function* analyzeSubmissionWithStreaming({
  textSubmission,
  fileSystemTree,
  criterionDescription,
  criterionPoints,
  tools,
}: {
  submissionDir: string;
  textSubmission: string | null;
  fileSystemTree: string[];
  criterionDescription: string;
  criterionPoints: number;
  tools: AiTool[];
}): AsyncGenerator<
  ConversationMessage,
  { conversation: ConversationMessage[]; analysis: AnalysisResult }
> {
  console.log("ðŸš€ Starting streaming analysis...");

  const systemPrompt = `You are an expert academic evaluator analyzing a student submission against a specific rubric criterion.

RUBRIC CRITERION TO EVALUATE:
- Description: ${criterionDescription}
- Maximum Points: ${criterionPoints}

STUDENT SUBMISSION OVERVIEW:
File System Structure:
${fileSystemTree}

${
  textSubmission
    ? `Text Submission Content:
${textSubmission}
`
    : "No text submission found."
}

AVAILABLE TOOLS:
- get_file_system_tree: Get the complete file system structure
- read_file: Read specific files from the submission

EVALUATION PROCESS:
1. Start by examining the file system structure and any text submission provided
2. Use the read_file tool to examine relevant files that might contain evidence for the criterion
3. You can call tools multiple times to explore different files as needed
4. For text files, pay attention to line numbers when referencing specific content
5. For PDFs, reference specific pages when citing evidence
6. For images, note their presence and relevance even though content can't be analyzed
7. Focus on concrete evidence and provide confidence levels for your assessments

${evidenceSchemaPrompt}

Take your time to thoroughly explore the submission before providing your final structured analysis.

Use the available tools to explore the submission thoroughly. When you have gathered sufficient evidence and no longer need to call any tools, your next response will be interpreted as your final structured analysis.

Analyze how well this submission meets the rubric criterion. Provide a confidence level, recommended points, brief description of your assessment, and cite specific evidence from the files you examined.

CRITICAL: Only include evidence entries for actual files you have examined with the read_file tool. Do not create evidence entries based solely on the file system structure or absence of files.
`;

  const messages: ConversationMessage[] = [
    { role: "system", content: systemPrompt },
    {
      role: "user",
      content: `Please analyze this student submission against the rubric criterion. Start by examining the file system structure and any provided text submission, then use the available tools to read additional files as needed. When you have gathered sufficient evidence, provide your final structured analysis in JSON format without making any more tool calls.`,
    },
  ];

  console.log("ðŸ“¨ Created initial messages, starting AI conversation...");

  // Create the generator
  const generator = getRubricAnalysisConversation({
    startingMessages: [...messages],
    tools,
    resultSchema: AnalysisResultSchema,
  });

  console.log("ðŸ”„ Generator created, beginning message iteration...");
  let messageCount = 0;

  // Yield all messages as they're generated
  for await (const message of generator) {
    messageCount++;
    console.log(`ðŸ“¤ Yielding message ${messageCount}: ${message.role}`);
    messages.push(message);
    yield message;
  }

  console.log(
    "ðŸ”š Generator has finished yielding messages.",
    messages,
    messages[messages.length - 1]
  );

  const parsed = parseResultFromMessage(
    messages[messages.length - 1],
    AnalysisResultSchema
  );
  const analysis: AnalysisResult = {
    ...parsed,
    evidence: parsed.evidence || [],
  };

  return { conversation: messages, analysis };
}

// Helper function to handle rubric analysis errors
export function handleRubricAnalysisError(error: unknown) {
  console.error("Error analyzing rubric criterion:", error);

  // Handle different types of errors with early returns
  if (!(error instanceof Error)) {
    throw new Error(`Failed to analyze rubric criterion: ${String(error)}`);
  }

  const message = error.message;

  // OpenAI API schema validation errors
  if (message.includes("Invalid schema for response_format")) {
    console.error("OpenAI Schema Validation Error:");
    console.error(
      "This indicates a mismatch between our Zod schema and OpenAI's strict mode requirements"
    );
    console.error(
      "All properties in the schema must be in the 'required' array for strict mode"
    );
    throw new Error(
      "AI service schema validation failed. Please check that all schema properties are marked as required for OpenAI strict mode."
    );
  }

  // OpenAI API rate limiting
  if (message.includes("429") || message.includes("rate")) {
    throw new Error(
      "AI service rate limit exceeded. Please try again in a few moments."
    );
  }

  // OpenAI API authentication errors
  if (message.includes("401") || message.includes("authentication")) {
    throw new Error(
      "AI service authentication failed. Please check your API key configuration."
    );
  }

  // JSON parsing errors
  if (message.includes("Invalid analysis format")) {
    throw new Error(
      "AI service returned invalid response format. The response could not be parsed as valid JSON matching the expected schema."
    );
  }

  // File system errors
  if (message.includes("ENOENT") || message.includes("no such file")) {
    throw new Error(
      "Submission files not found. Please ensure the student has submitted their assignment and files have been processed."
    );
  }

  // Network or timeout errors
  if (message.includes("timeout") || message.includes("ECONNRESET")) {
    throw new Error(
      "Network timeout while contacting AI service. Please try again."
    );
  }

  // Generic fallback with original error details
  throw new Error(`Failed to analyze rubric criterion: ${message}`);
}

// Save the evaluation results to a JSON file
export async function saveEvaluationResults({
  courseId,
  assignmentId,
  studentName,
  criterionId,
  criterionDescription,
  conversationMessages,
  analysis,
  termName,
  courseName,
  assignmentName,
}: {
  courseId: number;
  assignmentId: number;
  studentName: string;
  criterionId: string | undefined;
  criterionDescription: string;
  conversationMessages: ConversationMessage[];
  analysis: AnalysisResult;
  termName: string;
  courseName: string;
  assignmentName: string;
}) {
  try {
    const timestamp = new Date().toISOString().replace(/[:.]/g, "-");
    const sanitizedStudentName = sanitizeName(studentName);
    const criterionIdSafe =
      criterionId || sanitizeName(criterionDescription).slice(0, 20);

    // Create filename: <student-name>.rubric.<rubriccriterionid>-<timestamp>.json
    const fileName = `${sanitizedStudentName}.rubric.${criterionIdSafe}-${timestamp}.json`;

    // Get the student submission directory to place the file next to it
    const submissionDir = getMetadataSubmissionDirectory({
      termName,
      courseName,
      assignmentId,
      assignmentName,
      studentName,
    });
    const evaluationFilePath = path.join(submissionDir, fileName);

    // Create the evaluation data using the proper schema
    const evaluationData: FullEvaluation = {
      filePath: evaluationFilePath,
      fileName,
      metadata: {
        courseId,
        assignmentId,
        studentName,
        criterionId,
        criterionDescription,
        timestamp: new Date().toISOString(),
        model: aiModel,
      },
      conversation: conversationMessages.map((msg) => {
        const baseMessage = {
          role: msg.role as "system" | "user" | "assistant" | "tool",
          content: typeof msg.content === "string" ? msg.content : undefined,
        };

        // Handle tool calls for assistant messages
        if (msg.role === "assistant" && "tool_calls" in msg && msg.tool_calls) {
          return {
            ...baseMessage,
            tool_calls: msg.tool_calls.map((tc) => ({
              id: tc.id,
              type: tc.type,
              function:
                "function" in tc && tc.function
                  ? {
                      name: tc.function.name,
                      arguments: tc.function.arguments,
                    }
                  : undefined,
            })),
          };
        }

        // Handle tool call id for tool messages
        if (msg.role === "tool" && "tool_call_id" in msg) {
          return {
            ...baseMessage,
            tool_call_id: msg.tool_call_id,
          };
        }

        return baseMessage;
      }),
      evaluation: analysis,
      submissionPath: submissionDir,
    };

    // Validate the data against the schema before saving
    const validatedData = parseSchema(
      FullEvaluationSchema,
      evaluationData,
      "Full evaluation data"
    );

    // Ensure the directory exists
    fs.mkdirSync(path.dirname(evaluationFilePath), { recursive: true });

    // Write the evaluation results
    fs.writeFileSync(
      evaluationFilePath,
      JSON.stringify(validatedData, null, 2),
      "utf-8"
    );

    console.log(`Saved evaluation results to: ${evaluationFilePath}`);
  } catch (error) {
    console.error("Error saving evaluation results:", error);
    // Don't throw - this shouldn't break the main analysis
  }
}
